{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd05cc-909e-4f44-97d2-c4c02d09318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd//A2\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Load your dataset\n",
    "data_path = '/mnt/data/image_datasets_1.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming 'label' is the target and the rest are features\n",
    "X = df.drop('label', axis=1)  # Replace 'label' with the actual name of the label column\n",
    "y = df['label']               # Replace 'label' with the actual name of the label column\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define parameter grids\n",
    "perceptron_param_grid = {\n",
    "    'penalty': [None, 'l2', 'l1', 'elasticnet'],\n",
    "    'alpha': uniform(0.0001, 0.1),\n",
    "    'max_iter': randint(100, 1000),\n",
    "    'tol': [1e-4, 1e-3, 1e-2],\n",
    "    'shuffle': [True, False]\n",
    "}\n",
    "\n",
    "mlp_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': uniform(0.0001, 0.1),\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "perceptron = Perceptron()\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "# RandomizedSearchCV for Perceptron\n",
    "random_search_perceptron = RandomizedSearchCV(perceptron, perceptron_param_grid, n_iter=20, cv=5, random_state=42, n_jobs=-1)\n",
    "random_search_perceptron.fit(X_train, y_train)\n",
    "\n",
    "# RandomizedSearchCV for MLPClassifier\n",
    "random_search_mlp = RandomizedSearchCV(mlp, mlp_param_grid, n_iter=20, cv=5, random_state=42, n_jobs=-1)\n",
    "random_search_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters and test the models\n",
    "print(\"Best Hyperparameters for Perceptron:\", random_search_perceptron.best_params_)\n",
    "print(\"Best Hyperparameters for MLPClassifier:\", random_search_mlp.best_params_)\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred_perceptron = random_search_perceptron.predict(X_test)\n",
    "y_pred_mlp = random_search_mlp.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report for Perceptron:\")\n",
    "print(classification_report(y_test, y_pred_perceptron))\n",
    "\n",
    "print(\"\\nClassification Report for MLPClassifier:\")\n",
    "print(classification_report(y_test, y_pred_mlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff75e6-c445-48db-9469-553009eb1825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd //A3\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Load your dataset\n",
    "data_path = '/mnt/data/image_datasets_1.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming 'label' is the target and the rest are features\n",
    "X = df.drop('label', axis=1)  # Replace 'label' with the actual name of the label column\n",
    "y = df['label']               # Replace 'label' with the actual name of the label column\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'SVM': SVC(probability=True),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'AdaBoost': AdaBoostClassifier(),\n",
    "    'XGBoost': XGBClassifier(eval_metric='logloss', use_label_encoder=False),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Perceptron': Perceptron(),\n",
    "    'MLP': MLPClassifier(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Define a function to compute multiple performance metrics\n",
    "def evaluate_model(y_test, y_pred, y_pred_proba=None):\n",
    "    metrics = {}\n",
    "    metrics['accuracy'] = accuracy_score(y_test, y_pred)\n",
    "    metrics['precision'] = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    metrics['recall'] = recall_score(y_test, y_pred, average='weighted')\n",
    "    metrics['f1_score'] = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    if y_pred_proba is not None:\n",
    "        metrics['roc_auc'] = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "    else:\n",
    "        metrics['roc_auc'] = None\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Initialize an empty results dataframe\n",
    "results = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC'])\n",
    "\n",
    "# Train, predict, and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # For models that support probability prediction\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        metrics = evaluate_model(y_test, y_pred, y_pred_proba)\n",
    "    else:\n",
    "        metrics = evaluate_model(y_test, y_pred)\n",
    "    \n",
    "    results = results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': metrics['accuracy'],\n",
    "        'Precision': metrics['precision'],\n",
    "        'Recall': metrics['recall'],\n",
    "        'F1 Score': metrics['f1_score'],\n",
    "        'ROC AUC': metrics['roc_auc']\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f09507c-b816-40ee-92de-c757142deefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap ///01\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "data_path = '/mnt/data/image_datasets_1.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming 'label' is the target and the rest are features\n",
    "X = df.drop('label', axis=1)  # Replace 'label' with the actual name of the label column\n",
    "y = df['label']               # Replace 'label' with the actual name of the label column\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize a RandomForestClassifier\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize an XGBClassifier\n",
    "xgb_model = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# SHAP Explanation for RandomForestClassifier\n",
    "explainer_rf = shap.Explainer(rf_model, X_train)\n",
    "shap_values_rf = explainer_rf(X_test)\n",
    "\n",
    "# SHAP Explanation for XGBClassifier\n",
    "explainer_xgb = shap.Explainer(xgb_model, X_train)\n",
    "shap_values_xgb = explainer_xgb(X_test)\n",
    "\n",
    "# Plot SHAP summary plot for RandomForest\n",
    "shap.summary_plot(shap_values_rf, X_test, feature_names=X.columns)\n",
    "\n",
    "# Plot SHAP summary plot for XGBoost\n",
    "shap.summary_plot(shap_values_xgb, X_test, feature_names=X.columns)\n",
    "\n",
    "# SHAP feature importance bar plot for RandomForest\n",
    "shap.plots.bar(shap_values_rf, feature_names=X.columns)\n",
    "\n",
    "# SHAP feature importance bar plot for XGBoost\n",
    "shap.plots.bar(shap_values_xgb, feature_names=X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6461425a-378e-481a-9617-f6de4a5d40f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd //02\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Load your dataset\n",
    "data_path = '/mnt/data/image_datasets_1.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming 'label' is the target and the rest are features\n",
    "X = df.drop('label', axis=1)  # Replace 'label' with the actual name of the label column\n",
    "y = df['label']               # Replace 'label' with the actual name of the label column\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a RandomForestClassifier\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize LIME Explainer\n",
    "explainer = LimeTabularExplainer(X_train, feature_names=df.columns[:-1], class_names=np.unique(y),\n",
    "                                 mode='classification', discretize_continuous=True)\n",
    "\n",
    "# Choose a specific instance from the test set to explain\n",
    "instance = X_test[0].reshape(1, -1)\n",
    "\n",
    "# Generate explanation for the chosen instance\n",
    "exp = explainer.explain_instance(instance.flatten(), rf_model.predict_proba, num_features=5)\n",
    "\n",
    "# Show the explanation in text\n",
    "exp.show_in_notebook(show_table=True)\n",
    "\n",
    "# Or you can display the explanation as a graph\n",
    "exp.as_pyplot_figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c09042-687f-459e-9725-277a9cc6bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd //03\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "# Load your dataset\n",
    "data_path = '/mnt/data/image_datasets_1.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming 'label' is the target and the rest are features\n",
    "X = df.drop('label', axis=1)  # Replace 'label' with the actual name of the label column\n",
    "y = df['label']               # Replace 'label' with the actual name of the label column\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Initialize a RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Hyperparameter grids\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=10, \n",
    "                                   cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "random_search.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "random_search_time = end_time - start_time\n",
    "random_best_params = random_search.best_params_\n",
    "random_best_score = random_search.best_score_\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "grid_search_time = end_time - start_time\n",
    "grid_best_params = grid_search.best_params_\n",
    "grid_best_score = grid_search.best_score_\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_random = random_search.predict(X_test)\n",
    "y_pred_grid = grid_search.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "random_accuracy = accuracy_score(y_test, y_pred_random)\n",
    "grid_accuracy = accuracy_score(y_test, y_pred_grid)\n",
    "\n",
    "random_f1 = f1_score(y_test, y_pred_random, average='weighted')\n",
    "grid_f1 = f1_score(y_test, y_pred_grid, average='weighted')\n",
    "\n",
    "# Tabulate the results\n",
    "results = pd.DataFrame({\n",
    "    'Search Method': ['RandomizedSearchCV', 'GridSearchCV'],\n",
    "    'Best Score (CV)': [random_best_score, grid_best_score],\n",
    "    'Best Params': [random_best_params, grid_best_params],\n",
    "    'Test Accuracy': [random_accuracy, grid_accuracy],\n",
    "    'Test F1-Score': [random_f1, grid_f1],\n",
    "    'Search Time (seconds)': [random_search_time, grid_search_time]\n",
    "})\n",
    "\n",
    "print(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
